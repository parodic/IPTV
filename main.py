import urllib.request
from urllib.parse import quote, unquote
import re
import os
from datetime import datetime, timedelta, timezone
import opencc

# ===================== å…¨å±€æ ¸å¿ƒé…ç½® =====================
# æŒ‡å®šæŒ‰TXTæ–‡ä»¶å†…é¡ºåºæ’åˆ—çš„åˆ†ç±»ï¼Œå…¶ä½™è‡ªåŠ¨å­—å…¸åºæ’åºï¼ŒæŒ‰éœ€å¢åˆ 
ORDERED_CHANNEL_TYPES = ["å¤®è§†é¢‘é“", "å«è§†é¢‘é“", "æ¸¯æ¾³å°", "ç”µå½±é¢‘é“", "ç”µè§†å‰§é¢‘é“", "ç»¼è‰ºé¢‘é“", "åŸ‹å †å †"]
# é¢‘é“åç§°æ¸…ç†å­—ç¬¦é›†
REMOVAL_LIST = [
    "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[ipv6]", "[ipv4]", "_ç”µä¿¡", "ç”µä¿¡", "ï¼ˆHDï¼‰", "[è¶…æ¸…]",
    "é«˜æ¸…", "è¶…æ¸…", "-HD", "(HK)", "AKtv", "@", "IPV6", "ğŸï¸", "ğŸ¦", " ",
    "[BD]", "[VGA]", "[HD]", "[SD]", "(1080p)", "(720p)", "(480p)"
]
# ç½‘ç»œè¯·æ±‚é…ç½®
USER_AGENT = "PostmanRuntime-ApipostRuntime/1.1.0"
URL_FETCH_TIMEOUT = 10
# ç™½åå•æµ‹é€Ÿé˜ˆå€¼(ms)
RESPONSE_TIME_THRESHOLD = 2000
# M3Uç›¸å…³é…ç½®
TVG_URL = "https://github.com/CCSH/IPTV/raw/refs/heads/main/e.xml.gz"
LOGO_URL_TPL = "https://raw.githubusercontent.com/CCSH/IPTV/refs/heads/main/logo/{}.png"
# æ‰€æœ‰å•ä¸ªé¢‘é“æœ€å¤šä¿ç•™çš„æœ‰æ•ˆæºæ•°é‡ï¼Œå¯ç›´æ¥ä¿®æ”¹æ•°å­—ï¼ˆ-1=æ— é™åˆ¶ï¼‰
SINGLE_CHANNEL_MAX_COUNT = 20  

# ===================== é€šç”¨å·¥å…·å‡½æ•° =====================
def get_project_dirs() -> dict:
    script_abspath = os.path.abspath(__file__)
    root_dir = os.path.dirname(script_abspath)
    return {
        "root": root_dir,
        "blacklist_auto": os.path.join(root_dir, "assets/whitelist-blacklist/blacklist_auto.txt"),
        "whitelist_auto": os.path.join(root_dir, "assets/whitelist-blacklist/whitelist_auto.txt"),
        "blacklist_manual": os.path.join(root_dir, "assets/whitelist-blacklist/blacklist_manual.txt"),
        "whitelist_manual": os.path.join(root_dir, "assets/whitelist-blacklist/whitelist_manual.txt"),
        "corrections_name": os.path.join(root_dir, "assets/corrections_name.txt"),
        "urls": os.path.join(root_dir, "assets/urls.txt"),
        "main_channel": os.path.join(root_dir, "ä¸»é¢‘é“"),
        "local_channel": os.path.join(root_dir, "åœ°æ–¹å°")
    }

def read_txt(file_path: str, strip: bool = True, skip_empty: bool = True) -> list:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if strip:
                lines = [line.strip() for line in lines]
            if skip_empty:
                lines = [line for line in lines if line]
            return lines
    except FileNotFoundError:
        print(f"[ERROR] æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        print(f"[ERROR] è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
        return []

def write_txt(file_path: str, data: list or str) -> None:
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        if isinstance(data, list):
            data = '\n'.join([str(line) for line in data])
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(data)
        print(f"[SUCCESS] æ–‡ä»¶å†™å…¥æˆåŠŸ: {os.path.basename(file_path)}")
    except Exception as e:
        print(f"[ERROR] å†™å…¥æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")

def safe_quote_url(url: str) -> str:
    try:
        unquoted = unquote(url)
        return quote(unquoted, safe=':/?&=')
    except Exception:
        return url

def traditional_to_simplified(text: str) -> str:
    if not hasattr(traditional_to_simplified, "converter"):
        traditional_to_simplified.converter = opencc.OpenCC('t2s')
    return traditional_to_simplified.converter.convert(text) if text else ""

# ===================== é»‘åå•/çº é”™å­—å…¸å¤„ç† =====================
def load_blacklist(blacklist_auto_path: str, blacklist_manual_path: str) -> set:
    def _extract_black_urls(file_path):
        lines = read_txt(file_path)
        urls = []
        for line in lines:
            if "," in line:
                url = line.split(',')[1].strip()
                if url:
                    urls.append(url)
        return urls
    auto_urls = _extract_black_urls(blacklist_auto_path)
    manual_urls = _extract_black_urls(blacklist_manual_path)
    combined = set(auto_urls + manual_urls)
    print(f"[INFO] åˆå¹¶é»‘åå•URLæ•°: {len(combined)}")
    return combined

def load_corrections(corrections_path: str) -> dict:
    corrections = {}
    lines = read_txt(corrections_path)
    for line in lines:
        if not line or "," not in line:
            continue
        parts = line.split(',')
        correct_name = parts[0].strip()
        for wrong_name in parts[1:]:
            wrong_name = wrong_name.strip()
            if wrong_name:
                corrections[wrong_name] = correct_name
    print(f"[INFO] åŠ è½½é¢‘é“çº é”™è§„åˆ™æ•°: {len(corrections)}")
    return corrections

# ===================== é¢‘é“åç§°/URLå¤„ç† =====================
def clean_channel_name(name: str) -> str:
    if not name:
        return ""
    for item in REMOVAL_LIST:
        name = name.replace(item, "")
    name = name.replace("CCTV-", "CCTV")
    name = name.replace("CCTV0", "CCTV")
    name = name.replace("PLUS", "+")
    name = name.replace("NewTV-", "NewTV")
    name = name.replace("iHOT-", "iHOT")
    name = name.replace("NEW", "New")
    name = name.replace("New_", "New")
    return name.strip()

def clean_url(url: str) -> str:
    if not url:
        return ""
    dollar_idx = url.rfind('$')
    return url[:dollar_idx].strip() if dollar_idx != -1 else url.strip()

def correct_channel_name(name: str, corrections: dict) -> str:
    if not name or name not in corrections:
        return name
    return corrections[name] if corrections[name] != name else name

# ===================== é¢‘é“å­—å…¸åŠ è½½ =====================
def load_channel_dictionaries(main_dir: str, local_dir: str) -> tuple[dict, dict, list]:
    main_channels = {
        "å¤®è§†é¢‘é“": "å¤®è§†é¢‘é“.txt", "å«è§†é¢‘é“": "å«è§†é¢‘é“.txt", "ä½“è‚²é¢‘é“": "ä½“è‚²é¢‘é“.txt",
        "ç”µå½±é¢‘é“": "ç”µå½±.txt", "ç”µè§†å‰§é¢‘é“": "ç”µè§†å‰§.txt", "æ¸¯æ¾³å°": "æ¸¯æ¾³å°.txt",
        "å›½é™…å°": "å›½é™…å°.txt", "çºªå½•ç‰‡": "çºªå½•ç‰‡.txt", "æˆæ›²é¢‘é“": "æˆæ›²é¢‘é“.txt",
        "è§£è¯´é¢‘é“": "è§£è¯´é¢‘é“.txt", "æ˜¥æ™š": "æ˜¥æ™š.txt", "NewTV": "NewTV.txt",
        "iHOT": "iHOT.txt", "å„¿ç«¥é¢‘é“": "å„¿ç«¥é¢‘é“.txt", "ç»¼è‰ºé¢‘é“": "ç»¼è‰ºé¢‘é“.txt",
        "åŸ‹å †å †": "åŸ‹å †å †.txt", "éŸ³ä¹é¢‘é“": "éŸ³ä¹é¢‘é“.txt", "æ¸¸æˆé¢‘é“": "æ¸¸æˆé¢‘é“.txt",
        "æ”¶éŸ³æœºé¢‘é“": "æ”¶éŸ³æœºé¢‘é“.txt", "ç›´æ’­ä¸­å›½": "ç›´æ’­ä¸­å›½.txt", "MTV": "MTV.txt",
        "å’ªå’•ç›´æ’­": "å’ªå’•ç›´æ’­.txt"
    }
    local_channels = {
        "ä¸Šæµ·é¢‘é“": "ä¸Šæµ·é¢‘é“.txt", "æµ™æ±Ÿé¢‘é“": "æµ™æ±Ÿé¢‘é“.txt", "æ±Ÿè‹é¢‘é“": "æ±Ÿè‹é¢‘é“.txt",
        "å¹¿ä¸œé¢‘é“": "å¹¿ä¸œé¢‘é“.txt", "æ¹–å—é¢‘é“": "æ¹–å—é¢‘é“.txt", "å®‰å¾½é¢‘é“": "å®‰å¾½é¢‘é“.txt",
        "æµ·å—é¢‘é“": "æµ·å—é¢‘é“.txt", "å†…è’™é¢‘é“": "å†…è’™é¢‘é“.txt", "æ¹–åŒ—é¢‘é“": "æ¹–åŒ—é¢‘é“.txt",
        "è¾½å®é¢‘é“": "è¾½å®é¢‘é“.txt", "é™•è¥¿é¢‘é“": "é™•è¥¿é¢‘é“.txt", "å±±è¥¿é¢‘é“": "å±±è¥¿é¢‘é“.txt",
        "å±±ä¸œé¢‘é“": "å±±ä¸œé¢‘é“.txt", "äº‘å—é¢‘é“": "äº‘å—é¢‘é“.txt", "åŒ—äº¬é¢‘é“": "åŒ—äº¬é¢‘é“.txt",
        "é‡åº†é¢‘é“": "é‡åº†é¢‘é“.txt", "ç¦å»ºé¢‘é“": "ç¦å»ºé¢‘é“.txt", "ç”˜è‚ƒé¢‘é“": "ç”˜è‚ƒé¢‘é“.txt",
        "å¹¿è¥¿é¢‘é“": "å¹¿è¥¿é¢‘é“.txt", "è´µå·é¢‘é“": "è´µå·é¢‘é“.txt", "æ²³åŒ—é¢‘é“": "æ²³åŒ—é¢‘é“.txt",
        "æ²³å—é¢‘é“": "æ²³å—é¢‘é“.txt", "é»‘é¾™æ±Ÿé¢‘é“": "é»‘é¾™æ±Ÿé¢‘é“.txt", "å‰æ—é¢‘é“": "å‰æ—é¢‘é“.txt",
        "æ±Ÿè¥¿é¢‘é“": "æ±Ÿè¥¿é¢‘é“.txt", "å®å¤é¢‘é“": "å®å¤é¢‘é“.txt", "é’æµ·é¢‘é“": "é’æµ·é¢‘é“.txt",
        "å››å·é¢‘é“": "å››å·é¢‘é“.txt", "å¤©æ´¥é¢‘é“": "å¤©æ´¥é¢‘é“.txt", "æ–°ç–†é¢‘é“": "æ–°ç–†é¢‘é“.txt"
    }
    lite_sort = [
        "å¤®è§†é¢‘é“", "å«è§†é¢‘é“", "æ¸¯æ¾³å°", "ç”µå½±é¢‘é“", "ç”µè§†å‰§é¢‘é“", "ç»¼è‰ºé¢‘é“",
        "NewTV", "iHOT", "ä½“è‚²é¢‘é“", "å’ªå’•ç›´æ’­", "åŸ‹å †å †", "éŸ³ä¹é¢‘é“", "æ¸¸æˆé¢‘é“", "è§£è¯´é¢‘é“"
    ]

    main_dict = {}
    for chn_type, filename in main_channels.items():
        file_path = os.path.join(main_dir, filename)
        lines = read_txt(file_path)
        main_dict[chn_type] = lines
        print(f"[INFO] åŠ è½½ä¸»é¢‘é“ {chn_type}: {len(lines)} ä¸ª")

    local_dict = {}
    for chn_type, filename in local_channels.items():
        file_path = os.path.join(local_dir, filename)
        lines = read_txt(file_path)
        local_dict[chn_type] = lines
        print(f"[INFO] åŠ è½½åœ°æ–¹å° {chn_type}: {len(lines)} ä¸ª")

    return main_dict, local_dict, lite_sort

# ===================== é¢‘é“åˆ†ç±»æ ¸å¿ƒ =====================
class ChannelClassifier:
    def __init__(self, main_dict: dict, local_dict: dict, blacklist: set):
        self.main_dict = main_dict
        self.local_dict = local_dict
        self.blacklist = blacklist
        self.channel_data = {}
        self.other_lines = []
        self.other_urls = set()
        self.all_urls = {}
        # === å…¨å±€å•é¢‘é“é™æµ æ–°å¢ï¼šå•é¢‘é“è®¡æ•°å­—å…¸ ===
        self.single_chn_count = {}  # key: é¢‘é“å(å¦‚CCTV1), value: å·²æ·»åŠ æºæ•°é‡
        # åˆå§‹åŒ–åˆ†ç±»æ•°æ®
        for chn_type in list(main_dict.keys()) + list(local_dict.keys()):
            self.channel_data[chn_type] = []
            self.all_urls[chn_type] = set()

    def check_url_exist(self, chn_type: str, url: str) -> bool:
        if url in self.all_urls.get(chn_type, set()) or "127.0.0.1" in url:
            return True
        return False

    # === å…¨å±€å•é¢‘é“é™æµ ===
    def is_single_chn_limit(self, channel_name: str) -> bool:
        if SINGLE_CHANNEL_MAX_COUNT == -1:
            return False  # -1è¡¨ç¤ºæ— é™åˆ¶
        # è·å–è¯¥é¢‘é“å·²æ·»åŠ æ•°é‡ï¼Œé»˜è®¤0
        current_count = self.single_chn_count.get(channel_name, 0)
        # è¾¾åˆ°ä¸Šé™è¿”å›Trueï¼Œå¦åˆ™False
        if current_count >= SINGLE_CHANNEL_MAX_COUNT:
            return True
        return False

    def add_channel_line(self, chn_type: str, line: str, url: str):
        self.channel_data[chn_type].append(line)
        self.all_urls[chn_type].add(url)
        # === å…¨å±€å•é¢‘é“é™æµ æ–°å¢ï¼šæ›´æ–°å•é¢‘é“è®¡æ•° ===
        channel_name = line.split(',')[0].strip()
        self.single_chn_count[channel_name] = self.single_chn_count.get(channel_name, 0) + 1

    def add_other_line(self, line: str, url: str):
        if url not in self.other_urls and url not in self.blacklist:
            self.other_urls.add(url)
            self.other_lines.append(line)

    # === å…¨å±€å•é¢‘é“é™æµ ===
    def classify(self, channel_name: str, channel_url: str, line: str):
        # å…ˆåˆ¤æ–­ï¼šé»‘åå•/ç©ºURL â†’ è·³è¿‡ï¼›å•é¢‘é“è¾¾ä¸Šé™ â†’ è·³è¿‡
        if channel_url in self.blacklist or not channel_url or self.is_single_chn_limit(channel_name):
            return
        # åŸæœ‰åˆ†ç±»é€»è¾‘ä¸å˜
        for chn_type, chn_names in self.main_dict.items():
            if channel_name in chn_names and not self.check_url_exist(chn_type, channel_url):
                self.add_channel_line(chn_type, line, channel_url)
                return
        for chn_type, chn_names in self.local_dict.items():
            if channel_name in chn_names and not self.check_url_exist(chn_type, channel_url):
                self.add_channel_line(chn_type, line, channel_url)
                return
        self.add_other_line(line, channel_url)

    def get_channel_data(self, chn_type: str) -> list:
        return self.channel_data.get(chn_type, [])

    def get_all_other(self) -> list:
        return self.other_lines

# ===================== æ•°æ®å¤„ç†ä¸ç”Ÿæˆ =====================
def is_m3u_content(text: str) -> bool:
    if not text:
        return False
    first_line = text.strip().splitlines()[0].strip()
    return first_line.startswith("#EXTM3U")

def convert_m3u_to_txt(m3u_content: str) -> list:
    lines = [line.strip() for line in m3u_content.split('\n') if line.strip()]
    txt_lines, channel_name = [], ""
    for line in lines:
        if line.startswith("#EXTM3U"):
            continue
        elif line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        elif line.startswith(("http", "rtmp", "p3p")):
            if channel_name:
                txt_lines.append(f"{channel_name},{line}")
        elif "#genre#" not in line and "," in line and "://" in line:
            if re.match(r'^[^,]+,[^\s]+://[^\s]+$', line):
                txt_lines.append(line)
    return txt_lines

def process_remote_url(url: str, classifier: ChannelClassifier, corrections: dict):
    print(f"[PROCESS] æ‹‰å–è¿œç¨‹æº: {url}")
    classifier.other_lines.append(f"{url},#genre#")
    try:
        headers = {'User-Agent': USER_AGENT}
        req = urllib.request.Request(safe_quote_url(url), headers=headers)
        with urllib.request.urlopen(req, timeout=URL_FETCH_TIMEOUT) as resp:
            data = resp.read()
            text = None
            for encoding in ['utf-8', 'gbk', 'gb2312', 'iso-8859-1']:
                try:
                    text = data.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            if not text:
                print(f"[ERROR] è¿œç¨‹æº {url} è§£ç å¤±è´¥")
                return
            if is_m3u_content(text):
                lines = convert_m3u_to_txt(text)
            else:
                lines = [line.strip() for line in text.split('\n') if line.strip()]
        print(f"[PROCESS] è¿œç¨‹æº {url} æå–æœ‰æ•ˆè¡Œ: {len(lines)}")
        for line in lines:
            process_single_line(line, classifier, corrections)
        classifier.other_lines.append('\n')
    except Exception as e:
        print(f"[ERROR] å¤„ç†è¿œç¨‹æº {url} å¤±è´¥: {str(e)}")

def process_single_line(line: str, classifier: ChannelClassifier, corrections: dict):
    if "#genre#" in line or "#EXTINF:" in line or "," not in line or "://" not in line:
        return
    try:
        channel_name, channel_address = line.split(',', 1)
    except ValueError:
        return
    # é¢‘é“åæ ‡å‡†åŒ–ï¼ˆç®€ç¹è½¬æ¢â†’æ¸…ç†â†’çº é”™ï¼‰
    channel_name = traditional_to_simplified(channel_name)
    channel_name = clean_channel_name(channel_name)
    channel_name = correct_channel_name(channel_name, corrections)
    channel_address = clean_url(channel_address)
    new_line = f"{channel_name},{channel_address}"
    # ä¼ å…¥æ ‡å‡†åŒ–åçš„é¢‘é“ååšåˆ†ç±»ï¼ˆä¿è¯è®¡æ•°ç»Ÿä¸€ï¼‰
    classifier.classify(channel_name, channel_address, new_line)

def sort_channel_data(channel_data: list, chn_type: str, cfg_list: list) -> list:
    if not channel_data:
        return channel_data
    
    if chn_type in ORDERED_CHANNEL_TYPES:
        cfg_index_map = {cfg_name: idx for idx, cfg_name in enumerate(cfg_list)}
        def _ordered_key(line):
            name = line.split(',')[0] if ',' in line else ""
            return cfg_index_map.get(name, len(cfg_list))
        return sorted(channel_data, key=_ordered_key)
    else:
        def _dict_key(line):
            name = line.split(',')[0] if ',' in line else ""
            pure_name = re.sub(r'[^\w\u4e00-\u9fff]', '', name)
            return pure_name if pure_name else name
        return sorted(channel_data, key=_dict_key)

def generate_live_text(classifier: ChannelClassifier, main_dict: dict, lite_sort: list) -> tuple[list, list]:
    bj_time = datetime.now(timezone.utc) + timedelta(hours=8)
    formatted_time = bj_time.strftime("%Y%m%d %H:%M")
    version = f"{formatted_time},https://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8?contentid=2820180516001"
    header = ["æ›´æ–°æ—¶é—´,#genre#", version, '\n']

    # ç”Ÿæˆliteç‰ˆ
    lite_lines = header.copy()
    for chn_type in lite_sort:
        chn_data = classifier.get_channel_data(chn_type)
        sorted_data = sort_channel_data(chn_data, chn_type, main_dict[chn_type])
        lite_lines += [f"{chn_type},#genre#"] + sorted_data + ['\n']
    lite_lines = lite_lines[:-1] if lite_lines and lite_lines[-1] == '\n' else lite_lines

    # ç”Ÿæˆfullç‰ˆ
    full_lines = lite_lines.copy() + ['\n']
    full_other_types = [
        "å„¿ç«¥é¢‘é“", "å›½é™…å°", "çºªå½•ç‰‡", "æˆæ›²é¢‘é“", "ä¸Šæµ·é¢‘é“", "æ¹–å—é¢‘é“",
        "æ¹–åŒ—é¢‘é“", "å¹¿ä¸œé¢‘é“", "æµ™æ±Ÿé¢‘é“", "å±±ä¸œé¢‘é“", "æ±Ÿè‹é¢‘é“", "å®‰å¾½é¢‘é“",
        "æµ·å—é¢‘é“", "å†…è’™é¢‘é“", "è¾½å®é¢‘é“", "é™•è¥¿é¢‘é“", "å±±è¥¿é¢‘é“", "äº‘å—é¢‘é“",
        "åŒ—äº¬é¢‘é“", "é‡åº†é¢‘é“", "ç¦å»ºé¢‘é“", "ç”˜è‚ƒé¢‘é“", "å¹¿è¥¿é¢‘é“", "è´µå·é¢‘é“",
        "æ²³åŒ—é¢‘é“", "æ²³å—é¢‘é“", "é»‘é¾™æ±Ÿé¢‘é“", "å‰æ—é¢‘é“", "æ±Ÿè¥¿é¢‘é“", "å®å¤é¢‘é“",
        "é’æµ·é¢‘é“", "å››å·é¢‘é“", "å¤©æ´¥é¢‘é“", "æ–°ç–†é¢‘é“", "æ˜¥æ™š", "ç›´æ’­ä¸­å›½", "MTV", "æ”¶éŸ³æœºé¢‘é“"
    ]
    for chn_type in full_other_types:
        chn_data = classifier.get_channel_data(chn_type)
        sort_list = main_dict.get(chn_type, []) or classifier.local_dict.get(chn_type, [])
        sorted_data = sort_channel_data(chn_data, chn_type, sort_list)
        full_lines += [f"{chn_type},#genre#"] + sorted_data + ['\n']
    full_lines = full_lines[:-1] if full_lines and full_lines[-1] == '\n' else full_lines

    return full_lines, lite_lines

def make_m3u(txt_file: str, m3u_file: str, tvg_url: str, logo_tpl: str):
    try:
        if not os.path.exists(txt_file):
            print(f"[ERROR] M3Uæºæ–‡ä»¶ä¸å­˜åœ¨: {txt_file}")
            return
        m3u_content = f"#EXTM3U x-tvg-url=\"{tvg_url}\"\n"
        lines = read_txt(txt_file, strip=True, skip_empty=True)
        group_name = ""
        for line in lines:
            if "," not in line:
                continue
            parts = line.split(',', 1)
            if len(parts) != 2:
                continue
            if "#genre#" in parts[1]:
                group_name = parts[0].strip()
                continue
            channel_name, channel_url = parts[0].strip(), parts[1].strip()
            if not channel_url or "://" not in channel_url:
                continue
            logo_url = logo_tpl.format(channel_name)
            m3u_content += (
                f"#EXTINF:-1  tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\"  group-title=\"{group_name}\",{channel_name}\n"
                f"{channel_url}\n"
            )
        write_txt(m3u_file, m3u_content)
    except Exception as e:
        print(f"[ERROR] ç”ŸæˆM3Uå¤±è´¥ {m3u_file}: {str(e)}")

# ===================== ä¸»å‡½æ•°æ‰§è¡Œ =====================
if __name__ == "__main__":
    timestart = datetime.now()
    print(f"[START] ç¨‹åºå¼€å§‹æ‰§è¡Œ: {timestart.strftime('%Y%m%d %H:%M:%S')}")
    dirs = get_project_dirs()
    
    blacklist = load_blacklist(dirs["blacklist_auto"], dirs["blacklist_manual"])
    corrections = load_corrections(dirs["corrections_name"])
    main_dict, local_dict, lite_sort = load_channel_dictionaries(dirs["main_channel"], dirs["local_channel"])
    classifier = ChannelClassifier(main_dict, local_dict, blacklist)

    print(f"[PROCESS] å¤„ç†æ‰‹åŠ¨ç™½åå•")
    whitelist_manual = read_txt(dirs["whitelist_manual"])
    classifier.other_lines.append("ç™½åå•,#genre#")
    for line in whitelist_manual:
        process_single_line(line, classifier, corrections)

    print(f"[PROCESS] å¤„ç†è‡ªåŠ¨ç™½åå•ï¼ˆå“åº”æ—¶é—´<{RESPONSE_TIME_THRESHOLD}msï¼‰")
    whitelist_auto = read_txt(dirs["whitelist_auto"])
    classifier.other_lines.append("ç™½åå•æµ‹é€Ÿ,#genre#")
    for line in whitelist_auto:
        if "#genre#" in line or "," not in line or "://" not in line:
            continue
        parts = line.split(",")
        if len(parts) < 2:
            continue
        try:
            resp_time = float(parts[0].replace("ms", ""))
        except ValueError:
            resp_time = 60000
        if resp_time < RESPONSE_TIME_THRESHOLD:
            process_single_line(",".join(parts[1:]), classifier, corrections)

    print(f"[PROCESS] å¤„ç†è¿œç¨‹URLæº")
    urls = read_txt(dirs["urls"])
    for url in urls:
        if url.startswith("http"):
            process_remote_url(url, classifier, corrections)

    print(f"[GENERATE] ç”Ÿæˆlive.txt/live_lite.txt")
    live_full, live_lite = generate_live_text(classifier, main_dict, lite_sort)
    live_full_path = os.path.join(dirs["root"], "live.txt")
    live_lite_path = os.path.join(dirs["root"], "live_lite.txt")
    others_path = os.path.join(dirs["root"], "others.txt")
    write_txt(live_full_path, live_full)
    write_txt(live_lite_path, live_lite)
    write_txt(others_path, classifier.other_lines)

    print(f"[GENERATE] ç”ŸæˆM3Uæ–‡ä»¶")
    make_m3u(live_full_path, os.path.join(dirs["root"], "live.m3u"), TVG_URL, LOGO_URL_TPL)
    make_m3u(live_lite_path, os.path.join(dirs["root"], "live_lite.m3u"), TVG_URL, LOGO_URL_TPL)

    timeend = datetime.now()
    elapsed = timeend - timestart
    minutes, seconds = int(elapsed.total_seconds() // 60), int(elapsed.total_seconds() % 60)
    blacklist_count = len(blacklist)
    live_count = len(live_full)
    others_count = len(classifier.other_lines)
    
    print("=" * 60)
    print(f"[END] ç¨‹åºæ‰§è¡Œå®Œæˆ: {timeend.strftime('%Y%m%d %H:%M:%S')}")
    print(f"[STAT] æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")
    print(f"[STAT] live.txtè¡Œæ•°: {live_count}")
    print(f"[STAT] others.txtè¡Œæ•°: {others_count}")
    print("=" * 60)



